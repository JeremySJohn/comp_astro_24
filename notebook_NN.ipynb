{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import ndimage, fft\n",
    "from sklearn.preprocessing import normalize, StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "class LightFluxProcessor:\n",
    "\n",
    "    def __init__(self, fourier=True, normalize=True, gaussian=True, standardize=True):\n",
    "        self.fourier = fourier\n",
    "        self.normalize = normalize\n",
    "        self.gaussian = gaussian\n",
    "        self.standardize = standardize\n",
    "\n",
    "    def fourier_transform(self, X):\n",
    "        return np.abs(fft.fft(X, n=X.size))\n",
    "\n",
    "    def process(self, df_train_x, df_dev_x):\n",
    "        # Apply fourier transform\n",
    "        if self.fourier:\n",
    "            print(\"Applying Fourier...\")\n",
    "            shape_train = df_train_x.shape\n",
    "            shape_dev = df_dev_x.shape\n",
    "            df_train_x = df_train_x.apply(self.fourier_transform, axis=1)\n",
    "            df_dev_x = df_dev_x.apply(self.fourier_transform, axis=1)\n",
    "\n",
    "            df_train_x_build = np.zeros(shape_train)\n",
    "            df_dev_x_build = np.zeros(shape_dev)\n",
    "\n",
    "            for ii, x in enumerate(df_train_x):\n",
    "                df_train_x_build[ii] = x\n",
    "\n",
    "            for ii, x in enumerate(df_dev_x):\n",
    "                df_dev_x_build[ii] = x\n",
    "\n",
    "            df_train_x = pd.DataFrame(df_train_x_build)\n",
    "            df_dev_x = pd.DataFrame(df_dev_x_build)\n",
    "\n",
    "            # Keep first half of data as it is symmetrical after previous steps\n",
    "            df_train_x = df_train_x.iloc[:, : (df_train_x.shape[1] // 2)].values\n",
    "            df_dev_x = df_dev_x.iloc[:, : (df_dev_x.shape[1] // 2)].values\n",
    "\n",
    "        # Normalize\n",
    "        if self.normalize:\n",
    "            print(\"Normalizing...\")\n",
    "            df_train_x = pd.DataFrame(normalize(df_train_x))\n",
    "            df_dev_x = pd.DataFrame(normalize(df_dev_x))\n",
    "\n",
    "            # df_train_x = df_train_x.div(df_train_x.sum(axis=1), axis=0)\n",
    "            # df_dev_x = df_dev_x.div(df_dev_x.sum(axis=1), axis=0)\n",
    "\n",
    "        # Gaussian filter to smooth out data\n",
    "        if self.gaussian:\n",
    "            print(\"Applying Gaussian Filter...\")\n",
    "            df_train_x = ndimage.filters.gaussian_filter(df_train_x, sigma=10)\n",
    "            df_dev_x = ndimage.filters.gaussian_filter(df_dev_x, sigma=10)\n",
    "\n",
    "        if self.standardize:\n",
    "            # Standardize X data\n",
    "            print(\"Standardizing...\")\n",
    "            std_scaler = StandardScaler()\n",
    "            df_train_x = std_scaler.fit_transform(df_train_x)\n",
    "            df_dev_x = std_scaler.transform(df_dev_x)\n",
    "\n",
    "        print(\"Finished Processing!\")\n",
    "        return df_train_x, df_dev_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Applying Fourier...\n",
      "Normalizing...\n",
      "Applying Gaussian Filter...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin\\AppData\\Local\\Temp\\ipykernel_16616\\1559398079.py:55: DeprecationWarning: Please import `gaussian_filter` from the `scipy.ndimage` namespace; the `scipy.ndimage.filters` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  df_train_x = ndimage.filters.gaussian_filter(df_train_x, sigma=10)\n",
      "C:\\Users\\Kevin\\AppData\\Local\\Temp\\ipykernel_16616\\1559398079.py:56: DeprecationWarning: Please import `gaussian_filter` from the `scipy.ndimage` namespace; the `scipy.ndimage.filters` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  df_dev_x = ndimage.filters.gaussian_filter(df_dev_x, sigma=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing...\n",
      "Finished Processing!\n",
      "X_train.shape:  (5087, 1598)\n",
      "Y_train.shape:  (5087, 1)\n",
      "X_dev.shape:  (570, 1598)\n",
      "Y_dev.shape:  (570, 1)\n",
      "n_x:  1598\n",
      "num_examples:  5087\n",
      "n_y:  1\n",
      "Training...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kevin\\anaconda3\\envs\\comp_astro\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py:750: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5159 - loss: 1.4733\n",
      "Epoch 2/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5078 - loss: 0.8611\n",
      "Epoch 3/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5068 - loss: 0.7646\n",
      "Epoch 4/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5049 - loss: 0.7341\n",
      "Epoch 5/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5011 - loss: 0.7186\n",
      "Epoch 6/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4855 - loss: 0.7109\n",
      "Epoch 7/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4946 - loss: 0.7007\n",
      "Epoch 8/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5060 - loss: 0.6950\n",
      "Epoch 9/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5015 - loss: 0.6972\n",
      "Epoch 10/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4972 - loss: 0.6993\n",
      "Epoch 11/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5052 - loss: 0.6968\n",
      "Epoch 12/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5153 - loss: 0.6931\n",
      "Epoch 13/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4963 - loss: 0.6967\n",
      "Epoch 14/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4960 - loss: 0.6972\n",
      "Epoch 15/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5069 - loss: 0.6968\n",
      "Epoch 16/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4966 - loss: 0.6949\n",
      "Epoch 17/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5143 - loss: 0.6942\n",
      "Epoch 18/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5011 - loss: 0.6954\n",
      "Epoch 19/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4943 - loss: 0.6945\n",
      "Epoch 20/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5117 - loss: 0.6935\n",
      "Epoch 21/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4974 - loss: 0.6950\n",
      "Epoch 22/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5068 - loss: 0.6950\n",
      "Epoch 23/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5007 - loss: 0.6935\n",
      "Epoch 24/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5043 - loss: 0.6930\n",
      "Epoch 25/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5055 - loss: 0.6939\n",
      "Epoch 26/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4957 - loss: 0.6942\n",
      "Epoch 27/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5048 - loss: 0.6937\n",
      "Epoch 28/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5075 - loss: 0.6931\n",
      "Epoch 29/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5043 - loss: 0.6950\n",
      "Epoch 30/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5028 - loss: 0.6935\n",
      "Epoch 31/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5009 - loss: 0.6941\n",
      "Epoch 32/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5093 - loss: 0.6934\n",
      "Epoch 33/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5008 - loss: 0.6935\n",
      "Epoch 34/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5166 - loss: 0.6935\n",
      "Epoch 35/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5158 - loss: 0.6918\n",
      "Epoch 36/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4951 - loss: 0.6944\n",
      "Epoch 37/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4989 - loss: 0.6934\n",
      "Epoch 38/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5022 - loss: 0.6938\n",
      "Epoch 39/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4971 - loss: 0.6944\n",
      "Epoch 40/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5153 - loss: 0.6924\n",
      "Epoch 41/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5066 - loss: 0.6930\n",
      "Epoch 42/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5149 - loss: 0.6940\n",
      "Epoch 43/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5263 - loss: 0.6923\n",
      "Epoch 44/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5103 - loss: 0.6938\n",
      "Epoch 45/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4971 - loss: 0.6930\n",
      "Epoch 46/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4987 - loss: 0.6931\n",
      "Epoch 47/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4927 - loss: 0.6951\n",
      "Epoch 48/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4972 - loss: 0.6933\n",
      "Epoch 49/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5077 - loss: 0.6930\n",
      "Epoch 50/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5020 - loss: 0.6932\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 984us/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "train set error 0.5280125810890506\n",
      "dev set error 0.5543859649122806\n",
      "------------\n",
      "precision_train 0.5804967801287948\n",
      "precision_dev 0.0\n",
      "------------\n",
      "recall_train 0.22055225445648374\n",
      "recall_dev 0.0\n",
      "------------\n",
      "confusion_matrix_train\n",
      "[[1770  456]\n",
      " [2230  631]]\n",
      "confusion_matrix_dev\n",
      "[[254   0]\n",
      " [316   0]]\n",
      "------------\n",
      "Train Set Positive Predictions 1087\n",
      "Dev Set Positive Predictions 0\n",
      "------------\n",
      "All 0's error train set 0.007273442107332416\n",
      "All 0's error dev set 0.008771929824561403\n",
      "------------\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kevin\\anaconda3\\envs\\comp_astro\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import keras\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Activation, Dropout\n",
    "\n",
    "# # from keras.layers.normalization import BatchNormalization\n",
    "# from keras import metrics\n",
    "\n",
    "# from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from scipy import ndimage, fft\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# from .preprocess_data import LightFluxProcessor\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "LOAD_MODEL = True  # continue training previous weights or start fresh\n",
    "RENDER_PLOT = False  # render loss and accuracy plots\n",
    "\n",
    "\n",
    "def build_network(shape):\n",
    "    model = tf.keras.models.Sequential(\n",
    "        #[\n",
    "        #    tf.keras.layers.Input(shape),\n",
    "        #    tf.keras.layers.Flatten(),\n",
    "        #    tf.keras.layers.Dense(1, activation=\"relu\"),\n",
    "        #    tf.keras.layers.Dropout(1, activation=\"relu\"),\n",
    "        #    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "        #]\n",
    "        # Here I add a Dropout layer as the assignment required in the part F. The humber of hidden layers is 3, and the number of neurons in each layer is 128, 64, 32 respectively.   \n",
    "        [\n",
    "            tf.keras.layers.Input(shape),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "        ]\n",
    "    )\n",
    "    loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "def np_X_Y_from_df(df):\n",
    "    df = shuffle(df)\n",
    "    df_X = df.drop([\"LABEL\"], axis=1)\n",
    "    X = np.array(df_X)\n",
    "    Y_raw = np.array(df[\"LABEL\"]).reshape((len(df[\"LABEL\"]), 1))\n",
    "    Y = Y_raw == 2\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_dataset_path = r\"C:\\Users\\Kevin\\OneDrive\\Documentos\\UNIPD\\2_year\\computational\\kepler\\data_injected\\exoTrain.csv\"\n",
    "    dev_dataset_path = r\"C:\\Users\\Kevin\\OneDrive\\Documentos\\UNIPD\\2_year\\computational\\kepler\\data_injected\\exoTest.csv\"\n",
    "\n",
    "    print(\"Loading datasets...\")\n",
    "    df_train = pd.read_csv(train_dataset_path, encoding=\"ISO-8859-1\")\n",
    "    df_dev = pd.read_csv(dev_dataset_path, encoding=\"ISO-8859-1\")\n",
    "\n",
    "    # Generate X and Y dataframe sets\n",
    "    df_train_x = df_train.drop(\"LABEL\", axis=1)\n",
    "    df_dev_x = df_dev.drop(\"LABEL\", axis=1)\n",
    "    df_train_y = df_train.LABEL\n",
    "    df_dev_y = df_dev.LABEL\n",
    "\n",
    "    # Process dataset\n",
    "    LFP = LightFluxProcessor(\n",
    "        fourier=True, normalize=True, gaussian=True, standardize=True\n",
    "    )\n",
    "    df_train_x, df_dev_x = LFP.process(df_train_x, df_dev_x)\n",
    "\n",
    "    # Rejoin X and Y\n",
    "    df_train_processed = pd.DataFrame(df_train_x).join(pd.DataFrame(df_train_y))\n",
    "    df_dev_processed = pd.DataFrame(df_dev_x).join(pd.DataFrame(df_dev_y))\n",
    "\n",
    "    # Load X and Y numpy arrays\n",
    "    X_train, Y_train = np_X_Y_from_df(df_train_processed)\n",
    "    X_dev, Y_dev = np_X_Y_from_df(df_dev_processed)\n",
    "\n",
    "    # Print data set stats\n",
    "    (num_examples, n_x) = (\n",
    "        X_train.shape\n",
    "    )  # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[1]  # n_y : output size\n",
    "    print(\"X_train.shape: \", X_train.shape)\n",
    "    print(\"Y_train.shape: \", Y_train.shape)\n",
    "    print(\"X_dev.shape: \", X_dev.shape)\n",
    "    print(\"Y_dev.shape: \", Y_dev.shape)\n",
    "    print(\"n_x: \", n_x)\n",
    "    print(\"num_examples: \", num_examples)\n",
    "    print(\"n_y: \", n_y)\n",
    "\n",
    "    # Build model\n",
    "    model = build_network(X_train.shape[1:])\n",
    "\n",
    "    # Load weights\n",
    "    load_path = \"\"\n",
    "    my_file = Path(load_path)\n",
    "    if LOAD_MODEL and my_file.is_file():\n",
    "        model.load_weights(load_path)\n",
    "        print(\"------------\")\n",
    "        print(\"Loaded saved weights\")\n",
    "        print(\"------------\")\n",
    "\n",
    "    sm = SMOTE()\n",
    "    X_train_sm, Y_train_sm = sm.fit_resample(X_train, Y_train)\n",
    "    # X_train_sm, Y_train_sm = X_train, Y_train\n",
    "\n",
    "    # Train\n",
    "    # checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    # callbacks_list = [checkpoint]\n",
    "    print(\"Training...\")\n",
    "    history = model.fit(X_train_sm, Y_train_sm, epochs=50, batch_size=32)\n",
    "\n",
    "    # Metrics\n",
    "    train_outputs = model.predict(X_train, batch_size=32)\n",
    "    dev_outputs = model.predict(X_dev, batch_size=32)\n",
    "    train_outputs = np.rint(train_outputs)\n",
    "    dev_outputs = np.rint(dev_outputs)\n",
    "    accuracy_train = accuracy_score(Y_train, train_outputs)\n",
    "    accuracy_dev = accuracy_score(Y_dev, dev_outputs)\n",
    "    precision_train = precision_score(Y_train, train_outputs)\n",
    "    precision_dev = precision_score(Y_dev, dev_outputs)\n",
    "    recall_train = recall_score(Y_train, train_outputs)\n",
    "    recall_dev = recall_score(Y_dev, dev_outputs)\n",
    "    confusion_matrix_train = confusion_matrix(Y_train, train_outputs)\n",
    "    confusion_matrix_dev = confusion_matrix(Y_dev, dev_outputs)\n",
    "\n",
    "    # # Save model\n",
    "    # print(\"Saving model...\")\n",
    "    # save_weights_path = \"checkpoints_v2/weights-recall-{}-{}.weights.h5\".format(\n",
    "    #     recall_train, recall_dev\n",
    "    # )  # load_path\n",
    "    # model.save_weights(save_weights_path)\n",
    "    # save_path = \"models_v2/model-recall-{}-{}.weights.h5\".format(\n",
    "    #     recall_train, recall_dev\n",
    "    # )  # load_path\n",
    "    # # model.save(save_path)\n",
    "\n",
    "    print(\"train set error\", 1.0 - accuracy_train)\n",
    "    print(\"dev set error\", 1.0 - accuracy_dev)\n",
    "    print(\"------------\")\n",
    "    print(\"precision_train\", precision_train)\n",
    "    print(\"precision_dev\", precision_dev)\n",
    "    print(\"------------\")\n",
    "    print(\"recall_train\", recall_train)\n",
    "    print(\"recall_dev\", recall_dev)\n",
    "    print(\"------------\")\n",
    "    print(\"confusion_matrix_train\")\n",
    "    print(confusion_matrix_train)\n",
    "    print(\"confusion_matrix_dev\")\n",
    "    print(confusion_matrix_dev)\n",
    "    print(\"------------\")\n",
    "    print(\"Train Set Positive Predictions\", np.count_nonzero(train_outputs))\n",
    "    print(\"Dev Set Positive Predictions\", np.count_nonzero(dev_outputs))\n",
    "    #  Predicting 0's will give you error:\n",
    "    print(\"------------\")\n",
    "    print(\"All 0's error train set\", 37 / 5087)\n",
    "    print(\"All 0's error dev set\", 5 / 570)\n",
    "\n",
    "    print(\"------------\")\n",
    "    print(\"------------\")\n",
    "\n",
    "    if RENDER_PLOT:\n",
    "        # list all data in history\n",
    "        print(history.history.keys())\n",
    "        # summarize history for accuracy\n",
    "        plt.plot(history.history[\"accuracy\"])\n",
    "        # plt.plot(history.history['val_acc'])\n",
    "        plt.title(\"model accuracy\")\n",
    "        plt.ylabel(\"accuracy\")\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "        plt.show()\n",
    "\n",
    "        # summarize history for loss\n",
    "        plt.plot(history.history[\"loss\"])\n",
    "        # plt.plot(history.history['val_loss'])\n",
    "        plt.title(\"model loss\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp_astro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
